{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4097e9c6-7f69-4e5d-8baa-9e7b36c91e75",
   "metadata": {},
   "source": [
    "# NNFS - Final Code per Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ebb026-c107-487b-99d1-53e5c35a6065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "\n",
    "# sets random seed to 0\n",
    "# Sets dtype default to float32\n",
    "# overrides np.dot()\n",
    "#nnfs.init()\n",
    "\n",
    "from nnfs.datasets import spiral_data\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e23cb-2a39-47dc-804f-723e040d7f8c",
   "metadata": {},
   "source": [
    "## Chapter Two Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9befd32c-bbc3-448b-b783-19803df0dfb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.8  ,  1.21 ,  2.385],\n",
       "       [ 8.9  , -1.81 ,  0.2  ],\n",
       "       [ 1.41 ,  1.051,  0.026]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "outputs = np.dot(inputs, np.asarray(weights).T) + biases\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d3f9a3-5696-43e0-85d8-25f3d5970eae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chapter 3 Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30637373-9a4d-49e7-ae26-0901d2b8e91c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize Weights & Biases\n",
    "        # set weights to be shape (n_inputs, n_neurons) so Matrix Product can be taken easily \n",
    "        # multiply by .01 to initialize non-zero weights small enough to minimize influence on training\n",
    "        self.weights = .01 * np.random.randn(n_inputs, n_neurons)\n",
    "        # one bias per neuron, initially set to zero\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    # Forward Pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) +  self.biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff35499-35ca-4297-b6bb-1d0544a2c672",
   "metadata": {},
   "source": [
    "## Chapter 4 Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a9414a-2e4d-429e-8ac7-59d10cdbe67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bed3c26a-f63e-471e-968a-ba7213c0d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    \n",
    "    # define forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Subtract max to help prevent overflow errors (exploding values)\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                        keepdims=True))\n",
    "        # Normalize for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                        keepdims=True)\n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e2990e-3691-42d5-9a4c-7cacb7bff337",
   "metadata": {},
   "source": [
    "## Chapter 5 Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cebe422c-3252-43bd-b54d-45dab0b62d22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \n",
    "    # Calculate data & regularization loss given model output & ground truth vals\n",
    "    def calculate(self, output, y):\n",
    "        \n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        \n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "220748d3-6032-4ef8-93dd-3177de452a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    \n",
    "    # Forward Pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # clip prediction \n",
    "        # min - to avoid taking the log of 0 and having a value of -inf\n",
    "        # max - to avoid log of 1 being negative/shifting confidence toward 1\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # If array is 1D, thus sparse (categorical)\n",
    "        # Get predictions at indices indicated in y_true\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "            \n",
    "        # If array is 2D, thus one hot encoded\n",
    "        # multiply prediction array by ground truth array & sum\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        # get the negative log of the predictions \n",
    "        neg_log_likelihoods = -np.log(correct_confidences)\n",
    "    \n",
    "        return neg_log_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfd0e80-a567-4c32-a77c-d34b42d709da",
   "metadata": {},
   "source": [
    "## Full Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b6d3c21-2b97-4132-a172-648466ae198a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.33333333, 0.33333333],\n",
       "       [0.3333333 , 0.33333316, 0.33333354],\n",
       "       [0.33333319, 0.3333326 , 0.33333421],\n",
       "       [0.33333323, 0.33333325, 0.33333352],\n",
       "       [0.33333317, 0.33333247, 0.33333436]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spiral Dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Initialize first hidden layer w/ 3 neurons\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Initialize ReLU Activation Function\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Initialize Second hidden layer w/ 3 neurons\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Initialize Softmax Activation Function\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Forward pass data through layer one\n",
    "dense1.forward(X)\n",
    "\n",
    "# Forward pass layer one output through ReLU Activation Function\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Forward pass output of ReLU through second dense layer\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Forward pass output of second dense layer through Softmax\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "activation2.output[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "516d0f6d-b980-4fa1-a83e-8f6a371e1eaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0986178263287691"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize loss function\n",
    "loss_function = Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# Perform forward pass through loss function\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33242167-c958-44ec-aa6b-ce9ad867be48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30333333333333334"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Accuracy for output of softmax & targets\n",
    "# Calculate values along first axis\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "\n",
    "# If y is 2D then convert values\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f3237-6e45-44b8-93d8-66e5cd7346e1",
   "metadata": {},
   "source": [
    "## Chapter 6 Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157608f9-ef4e-4724-846d-06a3fbcf9f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A fruitless attempt at optimization\n",
    "# Helper variables\n",
    "lowest_loss = 9999999 # arbitrary large initial value\n",
    "best_dense1_weights = dense1.weights.copy()\n",
    "best_dense1_biases = dense1.biases.copy()\n",
    "best_dense2_weights = dense2.weights.copy()\n",
    "best_dense2_biases = dense1.biases.copy()\n",
    "\n",
    "for i in range(10000):\n",
    "    # Generate a new set of weights for iteration\n",
    "    dense1.weights = 0.05 * np.random.randn(2, 3)\n",
    "    dense1.biases = 0.05 * np.random.randn(1, 3)\n",
    "    dense2.weights = 0.05 * np.random.randn(3, 3)\n",
    "    dense1.biases = 0.05 * np.random.randn(1, 3)\n",
    "    \n",
    "    # Forward pass of data through layers & activation functions\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output) #ReLU\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output) #softmax\n",
    "    \n",
    "    # get loss\n",
    "    loss = loss_function.calculate(activation2.output, y)\n",
    "    \n",
    "    # Get Accuracy for output of softmax & targets\n",
    "    # Calculate values along first axis\n",
    "    predictions = np.argmax(activation2.output, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    # if loss is less than current loss save values of weights/biases\n",
    "    if loss < lowest_loss:\n",
    "        print(f'New set of weights/biases found.\\n Iteration: {i} loss: {loss} accuracy: {accuracy}')\n",
    "        lowest_loss = loss\n",
    "        best_dense1_weights = dense1.weights.copy()\n",
    "        best_dense1_biases = dense1.biases.copy()\n",
    "        best_dense2_weights = dense2.weights.copy()\n",
    "        best_dense2_biases = dense1.biases.copy()\n",
    "    else:\n",
    "        dense1.weights = best_dense1_weights\n",
    "        dense1.biases = best_dense1_biases\n",
    "        dense2.weights = best_dense2_weights\n",
    "        dense1.biases = best_dense2_biases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
