{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143a36e3-f0bf-4e1e-ad46-fd5c8e8bd977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010c335d-3113-4693-a84b-a56a64d2562d",
   "metadata": {},
   "source": [
    "# Notes on Neural Networks from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fef684-85eb-4e3e-8bcc-5e2b64bcd21c",
   "metadata": {},
   "source": [
    "## Chapters 1 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89a73fd-ba14-472a-be27-c93eb0cd1d9e",
   "metadata": {},
   "source": [
    "### Terms\n",
    "\n",
    "### Data\n",
    "* Feature - A single measurable property of a phenomena that can be used for predictions or classifications.\n",
    "    - Usually Numeric. Features usually must be converted/encoded to numeric values to be used with ML algorithms.\n",
    "    - Can sometimes be combined to create new features. \n",
    "    - Should be informative and relevant to the task.\n",
    "    - Deep neural networks are able to learn complex relationships between features, including interactions and non-linear relationships. Thus eliminating the need for manual feature engineering.\n",
    "* Preprocessing Data - Adjusting feature data to be suitable for an algorithm.\n",
    "    - Encoding\n",
    "    - Normalization\n",
    "    - Scaling\n",
    "* Feature Set - Group of features. \n",
    "    - Represented as vectors.\n",
    "* Sample - An instance of a feature set. One \"Row\", one observation.\n",
    "* Batch - Group of samples.\n",
    "    - Seperated used to train with parallel processing.\n",
    "    - Batching helps prevent overfitting. See [this](https://nnfs.io/vyu/) animation\n",
    "* In-Sample Data - Training Data\n",
    "* Out-Of-Sample Data - Testing/Validation Data\n",
    "* Generalization - \"Generalizing\" the model to work well on never before seen input. \n",
    "* Overfitting - When a model \"memorizes\" a dataset and looks for \"too-specific\" features \n",
    "* Training - Slowly adjusting the weights/biases of a model to obtain the desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b5aed-0266-4a5d-8385-2baa1bc15232",
   "metadata": {},
   "source": [
    "### Model Types\n",
    "* Classification - Predict discrete labels/targets\n",
    "* Regression - Predict numeric values    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e1cfdb-f8fd-4d0c-a1ae-60436ecbc078",
   "metadata": {},
   "source": [
    "### Layers\n",
    "* Input Layer - Actual input data, typically preprocessed.\n",
    "* Output Layer - What the neural network returns\n",
    "    - Often has as many neurons as the training dataset has classes.\n",
    "* Dense Layer - Every neuron in each layer is connected to every neuron of the next layer.\n",
    "    - Each connection has a weight associated with it.\n",
    "\n",
    "### Networks\n",
    "* Fully Connected Network - Every neuron in the current layer is connected to every neuron from the previous layer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d894aa5b-3d0a-437f-8bd3-e9227c416473",
   "metadata": {},
   "source": [
    "### Activation\n",
    "* ```Input * Weight + Bias = Output```\n",
    "* Inputs * Weights (Connections) + bias (per neuron)\n",
    "* Weight - A trainable factor of how much of the input to use. \n",
    "    - Multiplied by the input value to determine how much of that input signal contributes to the output.\n",
    "    - Present in each *connection* between neurons.\n",
    "    - Weights are initially random, then are adjusted based on how far the actual output is from the desired output.\n",
    "    - Each connection in a neural network has a unique weight.\n",
    "    - Weights can be positive or negative.\n",
    "    - After all input * weight values are taken they are summed and added to the bias.\n",
    "* Bias - A constant added to the weighted sum of inputs which changes a neuron's activation threshold.\n",
    "    - Present in each neuron itself.\n",
    "    - Allows control of neuron activation threshold independent of the input values.\n",
    "    - Higher bias needs stronger signal for neuron to fire, lower bias means it will fire at a weaker signal\n",
    "    - Initially random/0, then are adjusted based on how far the actual output is from the desired output.\n",
    "* Activation Function - Fed the result of `input * weight + bias` resulting in whether the neuron should fire or not and the strength of the resulting signal.\n",
    "    - Sigmoid: Outputs a value between 0 and 1, representing a probability-like activation.\n",
    "    - ReLU (Rectified Linear Unit): Outputs the input directly if it's positive, otherwise outputs 0.\n",
    "    - Tanh (Hyperbolic Tangent): Outputs a value between -1 and 1, similar to sigmoid but with a steeper slope around zero.\n",
    "    - Activation functions introduce non-linearity into neural networks, which is crucial for them to learn complex patterns and relationships in data. Without non-linearity, networks would only be able to learn linear relationships, which is insufficient for most real-world tasks.\n",
    "    - Different activation functions have different properties like differentiability, vanishing/exploding gradient problems, and computational efficiency.\n",
    "    - Some activation functions work better for specific types of tasks or network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e06075-15f3-4f8d-b115-43c281749b42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Inputs\n",
    "inputs_vector = [1.0, 2.0, 3.0, 2.5]\n",
    "\n",
    "# Random Weights\n",
    "weights_vector = [0.2, 0.8, -0.5, 1.0]\n",
    "\n",
    "# Random Bias\n",
    "bias = 2.0\n",
    "\n",
    "# Get output for inputs to a single neuron\n",
    "def single_neuron_output(inputs, weights, bias):\n",
    "    weighted_sum = sum(inputs[i] * weights[i] for i in range(len(inputs)))\n",
    "    output = weighted_sum + bias\n",
    "    return output\n",
    "    \n",
    "single_neuron_output(inputs_vector, weights_vector, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9063f75f-79c1-4af9-82df-e63bde5fd6cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.8, 1.21, 2.385]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Inputs\n",
    "inputs_vector = [1.0, 2.0, 3.0, 2.5]\n",
    "\n",
    "# Random Weights\n",
    "weights_matrix = [[0.2, 0.8, -0.5, 1.0],\n",
    "                  [0.5, -0.91, 0.26, -0.5],\n",
    "                  [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "# Random Biases\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "# Get output for inputs of a single layer\n",
    "def single_layer_output(inputs, weights, biases):\n",
    "    outputs = [single_neuron_output(inputs, weights[i], biases[i]) for i in range(len(weights))]\n",
    "    return outputs\n",
    "\n",
    "single_layer_output(inputs_vector, weights_matrix, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d0d34-6fc5-4059-be89-b97010fc2643",
   "metadata": {},
   "source": [
    "### Vectors, Matrices, Tensors\n",
    "\n",
    "* Array - Homologous container. Homo - Same, Logos - Proportions/Ratios. Consistent in both dimensions. \n",
    "* Vector - Linear array; One dimensional array\n",
    "    - [1, 2, 3]\n",
    "* Row Vector - An array with the shape (1, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65d196eb-550c-4d80-b8e3-ae2116b90602",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cc196a9-d4a3-4180-91f7-200aba9a87b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a into a 2d row array\n",
    "np.expand_dims(a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb03bfd2-c72a-45dc-b9b0-fe212816b45b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a into a 2d row array\n",
    "np.array([a])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a831c10b-69be-43d3-84b9-50a19ad4e483",
   "metadata": {},
   "source": [
    "* Column Vector - An array with the shape (n, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a0badee-7b7d-49c6-b0bd-aa6dda23e5ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a into a 2d column array\n",
    "np.expand_dims(a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02f783c1-dbea-4c72-aabf-f87f9c357721",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transpose a into a column array\n",
    "np.array([a]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d3553a-fbae-41e7-9557-4f195528c311",
   "metadata": {},
   "source": [
    "* Matrix - Two dimensional Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e47f5af-d245-4aae-b880-230958cfaae2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2, 3],\n",
    "          [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519983b5-91e3-4001-aec1-e91efdc6477e",
   "metadata": {},
   "source": [
    "* Three dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f990cb9a-3e7d-4297-baf1-9f81fbda59ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1,  2,  3],\n",
       "        [ 4,  5,  6]],\n",
       "\n",
       "       [[ 7,  8,  9],\n",
       "        [10, 11, 12]],\n",
       "\n",
       "       [[13, 14, 15],\n",
       "        [16, 17, 18]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[[1, 2, 3],\n",
    "          [4, 5, 6]],\n",
    "         [[7, 8, 9],\n",
    "          [10, 11, 12]],\n",
    "         [[13, 14, 15],\n",
    "          [16, 17, 18]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2d7ee4-728c-4bc9-8a9d-52f59353d70c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "* Tensor - Object that can be represented as an array\n",
    "* Dot Product - Sum of the product of consecutive vector elements\n",
    "    - ```sum(v1[i] * v2[i] for in in range(len(v1)]```\n",
    "    - ```sum(inputs[i] * weights[i] for in in range(len(inputs)]```\n",
    "    - ```np.dot(inputs, weights)```\n",
    "    - Both vectors must be the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "382a0aeb-3770-4f6c-b078-3c56d35d648d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Inputs\n",
    "inputs_vector = [1.0, 2.0, 3.0, 2.5]\n",
    "\n",
    "# Random Weights\n",
    "weights_vector = [0.2, 0.8, -0.5, 1.0]\n",
    "\n",
    "# Random Bias\n",
    "bias = 2.0\n",
    "\n",
    "# Using NumPy dot product to get single neuron output\n",
    "output = np.dot(inputs_vector, weights_vector) + bias\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4cfa0d-2af1-4de5-96c7-fb60d71f90ae",
   "metadata": {},
   "source": [
    "* Matrix + vector  =  add vector to all rows/columns of matrix depending on if row/column vector\n",
    "    - Column vector must be same length as matrix column\n",
    "    - Row vector must be same length as matrix row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f4e9e82-6843-4e5f-a09b-a85ab4f61729",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1ee4b97-9ef3-486d-af03-7d9758777c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.zeros([3, 3])\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc814fe2-0d12-424b-bdae-1870377bd9ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B + np.asarray([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4597c1a2-e350-4475-8b57-fa8f37849e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [2., 2., 2.],\n",
       "       [3., 3., 3.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B + np.asarray([a]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb09057-1906-4081-8ac9-b3c0d352e938",
   "metadata": {},
   "source": [
    "* Matrix Product - The dot product of all combinations of rows of a first matrix (A) and columns of a second matrix (B).\n",
    "    - For a matrix A with dimensions x, y and a matrix B with dimensions n, m y must equal n to perform matrix product.\n",
    "    - A(x, y) & B(n, m) y == n\n",
    "    - Results in a Matrix C(x, m)\n",
    "    - The dot product of two vectors = a matrix product of a row and column vector.\n",
    "* Transposition - Changing a matrix A(x, y) to Matrix A(y, x). Flipping the columns into rows and rows into columns.\n",
    "    - Transposing weights matrix allows us to take matrix product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1856ef35-52b6-4b01-8f00-4e510c9b6f68",
   "metadata": {},
   "source": [
    "### Chapter Two Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f8d6b064-60b3-43a9-a519-b37419c42585",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.8  ,  1.21 ,  2.385],\n",
       "       [ 8.9  , -1.81 ,  0.2  ],\n",
       "       [ 1.41 ,  1.051,  0.026]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using numpy to get Matrix Product - transposing weights\n",
    "# Matrix of inputs requires matrix product\n",
    "inputs_matrix = [[1.0, 2.0, 3.0, 2.5],\n",
    "                 [2.0, 5.0, -1.0, 2.0],\n",
    "                 [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "# Random Weights\n",
    "weights_matrix = [[0.2, 0.8, -0.5, 1.0],\n",
    "                  [0.5, -0.91, 0.26, -0.5],\n",
    "                  [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "# Random Biases\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "# numpy converts biases to array by itself\n",
    "outputs = np.dot(inputs_matrix, np.asarray(weights_matrix).T) + biases\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ac7fd7-e35e-4d73-a66c-5fd74623f30e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac3fb5e-734d-4942-a844-4c0e757293b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
